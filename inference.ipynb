{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiitr/Analysis-and-Plotting-Tool-for-Delay-Model/blob/master/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vv5Oa3jPMXX"
      },
      "source": [
        "# Fish Speech"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the repo\n",
        "!git clone https://github.com/fishaudio/fish-speech\n",
        "\n",
        "#base linux pkgs\n",
        "!sudo apt-get install portaudio\n",
        "!sudo apt-get install gcc\n",
        "!sudo apt-get install portaudio19-dev python3-dev\n",
        "\n",
        "# python env buildding\n",
        "!pip install pip_tools\n",
        "!pip-compile fish-speech/pyproject.toml --output-file requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#linux var\n",
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "\n",
        "# change the version based on the latest released current 1.5\n",
        "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/\n",
        "\n",
        "# web gui (use public url as local does not work by passing app.launch(,share=True) in run_webui.py)\n",
        "# use compile for the Nvidia CUDA usage\n",
        "!python fish-speech/tools/run_webui.py \\\n",
        "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
        "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
        "    # --compile"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xtJ-VcpJPlVQ",
        "outputId": "2625df1a-4261-43ef-f241-bc81184fb5e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fish-speech'...\n",
            "remote: Enumerating objects: 5338, done.\u001b[K\n",
            "remote: Counting objects: 100% (1460/1460), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 5338 (delta 1244), reused 1211 (delta 1211), pack-reused 3878 (from 1)\u001b[K\n",
            "Receiving objects: 100% (5338/5338), 18.52 MiB | 11.37 MiB/s, done.\n",
            "Resolving deltas: 100% (3575/3575), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip_tools"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FkhW6E23P084",
        "outputId": "bd3589a4-9fb3-4bda-e4fa-ef8369b7f8fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip_tools\n",
            "  Downloading pip_tools-7.4.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting build>=1.0.0 (from pip_tools)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from pip_tools) (8.1.7)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.10/dist-packages (from pip_tools) (24.1.2)\n",
            "Collecting pyproject-hooks (from pip_tools)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pip_tools) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from pip_tools) (0.45.1)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from pip_tools) (2.2.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.0->pip_tools) (24.2)\n",
            "Downloading pip_tools-7.4.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject-hooks, build, pip_tools\n",
            "Successfully installed build-1.2.2.post1 pip_tools-7.4.1 pyproject-hooks-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd fish-speech/\n",
        "!pip-compile fish-speech/pyproject.toml --output-file requirements.txt\n",
        "!pip install -r fish-speech/requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KxL349VqQQTj",
        "outputId": "49e96c5a-daf6-47aa-cf10-6b044822db16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m649.0/649.0 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m743.0/743.0 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: --strip-extras is becoming the default in version 8.0.0. To silence this warning, either use --strip-extras to opt into the new default or use --no-strip-extras to retain the existing behavior.\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n",
            "\u001b[32m# This file is autogenerated by pip-compile with Python 3.10\u001b[0m\u001b[0m\n",
            "\u001b[32m# by the following command:\u001b[0m\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n",
            "\u001b[32m#    pip-compile --output-file=requirements.txt fish-speech/pyproject.toml\u001b[0m\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n",
            "absl-py==2.1.0\n",
            "    \u001b[32m# via tensorboard\u001b[0m\u001b[0m\n",
            "aiofiles==23.2.1\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "aiohappyeyeballs==2.4.4\n",
            "    \u001b[32m# via aiohttp\u001b[0m\u001b[0m\n",
            "aiohttp==3.11.11\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   fsspec\u001b[0m\u001b[0m\n",
            "aiosignal==1.3.2\n",
            "    \u001b[32m# via aiohttp\u001b[0m\u001b[0m\n",
            "aliyun-python-sdk-core==2.16.0\n",
            "    \u001b[32m# via\n",
            "    #   aliyun-python-sdk-kms\n",
            "    #   oss2\u001b[0m\u001b[0m\n",
            "aliyun-python-sdk-kms==2.16.5\n",
            "    \u001b[32m# via oss2\u001b[0m\u001b[0m\n",
            "annotated-types==0.7.0\n",
            "    \u001b[32m# via pydantic\u001b[0m\u001b[0m\n",
            "antlr4-python3-runtime==4.9.3\n",
            "    \u001b[32m# via\n",
            "    #   hydra-core\n",
            "    #   omegaconf\u001b[0m\u001b[0m\n",
            "anyio==4.7.0\n",
            "    \u001b[32m# via\n",
            "    #   gradio\n",
            "    #   httpx\n",
            "    #   starlette\u001b[0m\u001b[0m\n",
            "async-timeout==5.0.1\n",
            "    \u001b[32m# via aiohttp\u001b[0m\u001b[0m\n",
            "attrs==24.3.0\n",
            "    \u001b[32m# via aiohttp\u001b[0m\u001b[0m\n",
            "audioread==3.0.1\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "av==14.0.1\n",
            "    \u001b[32m# via faster-whisper\u001b[0m\u001b[0m\n",
            "baize==0.22.2\n",
            "    \u001b[32m# via kui\u001b[0m\u001b[0m\n",
            "cachetools==5.5.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "certifi==2024.12.14\n",
            "    \u001b[32m# via\n",
            "    #   httpcore\n",
            "    #   httpx\n",
            "    #   requests\n",
            "    #   sentry-sdk\u001b[0m\u001b[0m\n",
            "cffi==1.17.1\n",
            "    \u001b[32m# via\n",
            "    #   cryptography\n",
            "    #   soundfile\u001b[0m\u001b[0m\n",
            "charset-normalizer==3.4.0\n",
            "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
            "click==8.1.8\n",
            "    \u001b[32m# via\n",
            "    #   typer\n",
            "    #   uvicorn\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "coloredlogs==15.0.1\n",
            "    \u001b[32m# via onnxruntime\u001b[0m\u001b[0m\n",
            "crcmod==1.7\n",
            "    \u001b[32m# via oss2\u001b[0m\u001b[0m\n",
            "cryptography==44.0.0\n",
            "    \u001b[32m# via aliyun-python-sdk-core\u001b[0m\u001b[0m\n",
            "ctranslate2==4.5.0\n",
            "    \u001b[32m# via faster-whisper\u001b[0m\u001b[0m\n",
            "datasets==2.18.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "decorator==5.1.1\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "dill==0.3.8\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   multiprocess\u001b[0m\u001b[0m\n",
            "docker-pycreds==0.4.0\n",
            "    \u001b[32m# via wandb\u001b[0m\u001b[0m\n",
            "editdistance==0.8.1\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "einops==0.8.0\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   vector-quantize-pytorch\u001b[0m\u001b[0m\n",
            "einx[torch]==0.2.2\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   vector-quantize-pytorch\u001b[0m\u001b[0m\n",
            "exceptiongroup==1.2.2\n",
            "    \u001b[32m# via anyio\u001b[0m\u001b[0m\n",
            "fastapi==0.115.6\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "faster-whisper==1.1.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "ffmpy==0.5.0\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "filelock==3.16.1\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   huggingface-hub\n",
            "    #   torch\n",
            "    #   transformers\n",
            "    #   triton\u001b[0m\u001b[0m\n",
            "flatbuffers==24.12.23\n",
            "    \u001b[32m# via onnxruntime\u001b[0m\u001b[0m\n",
            "frozendict==2.4.6\n",
            "    \u001b[32m# via einx\u001b[0m\u001b[0m\n",
            "frozenlist==1.5.0\n",
            "    \u001b[32m# via\n",
            "    #   aiohttp\n",
            "    #   aiosignal\u001b[0m\u001b[0m\n",
            "fsspec[http]==2024.2.0\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   gradio-client\n",
            "    #   huggingface-hub\n",
            "    #   lightning\n",
            "    #   pytorch-lightning\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "funasr==1.1.5\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "gitdb==4.0.11\n",
            "    \u001b[32m# via gitpython\u001b[0m\u001b[0m\n",
            "gitpython==3.1.43\n",
            "    \u001b[32m# via wandb\u001b[0m\u001b[0m\n",
            "gradio==5.9.1\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "gradio-client==1.5.2\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "grpcio==1.68.1\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   tensorboard\u001b[0m\u001b[0m\n",
            "h11==0.14.0\n",
            "    \u001b[32m# via\n",
            "    #   httpcore\n",
            "    #   uvicorn\u001b[0m\u001b[0m\n",
            "httpcore==1.0.7\n",
            "    \u001b[32m# via httpx\u001b[0m\u001b[0m\n",
            "httpx==0.28.1\n",
            "    \u001b[32m# via\n",
            "    #   gradio\n",
            "    #   gradio-client\n",
            "    #   safehttpx\u001b[0m\u001b[0m\n",
            "huggingface-hub==0.27.0\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   faster-whisper\n",
            "    #   gradio\n",
            "    #   gradio-client\n",
            "    #   tokenizers\n",
            "    #   transformers\u001b[0m\u001b[0m\n",
            "humanfriendly==10.0\n",
            "    \u001b[32m# via coloredlogs\u001b[0m\u001b[0m\n",
            "hydra-core==1.3.2\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   funasr\u001b[0m\u001b[0m\n",
            "idna==3.10\n",
            "    \u001b[32m# via\n",
            "    #   anyio\n",
            "    #   httpx\n",
            "    #   requests\n",
            "    #   yarl\u001b[0m\u001b[0m\n",
            "jaconv==0.4.0\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "jamo==0.4.1\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "jieba==0.42.1\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "jinja2==3.1.5\n",
            "    \u001b[32m# via\n",
            "    #   gradio\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "jmespath==0.10.0\n",
            "    \u001b[32m# via aliyun-python-sdk-core\u001b[0m\u001b[0m\n",
            "joblib==1.4.2\n",
            "    \u001b[32m# via\n",
            "    #   librosa\n",
            "    #   pynndescent\n",
            "    #   scikit-learn\u001b[0m\u001b[0m\n",
            "kaldiio==2.18.0\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "kui==1.8.1\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "lazy-loader==0.4\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "librosa==0.10.2.post1\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   funasr\u001b[0m\u001b[0m\n",
            "lightning==2.5.0.post0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "lightning-utilities==0.11.9\n",
            "    \u001b[32m# via\n",
            "    #   lightning\n",
            "    #   pytorch-lightning\n",
            "    #   torchmetrics\u001b[0m\u001b[0m\n",
            "llvmlite==0.43.0\n",
            "    \u001b[32m# via\n",
            "    #   numba\n",
            "    #   pynndescent\u001b[0m\u001b[0m\n",
            "loguru==0.7.3\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "loralib==0.1.2\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "markdown==3.7\n",
            "    \u001b[32m# via tensorboard\u001b[0m\u001b[0m\n",
            "markdown-it-py==3.0.0\n",
            "    \u001b[32m# via rich\u001b[0m\u001b[0m\n",
            "markupsafe==2.1.5\n",
            "    \u001b[32m# via\n",
            "    #   gradio\n",
            "    #   jinja2\n",
            "    #   werkzeug\u001b[0m\u001b[0m\n",
            "mdurl==0.1.2\n",
            "    \u001b[32m# via markdown-it-py\u001b[0m\u001b[0m\n",
            "modelscope==1.17.1\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "mpmath==1.3.0\n",
            "    \u001b[32m# via sympy\u001b[0m\u001b[0m\n",
            "msgpack==1.1.0\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "multidict==6.1.0\n",
            "    \u001b[32m# via\n",
            "    #   aiohttp\n",
            "    #   yarl\u001b[0m\u001b[0m\n",
            "multiprocess==0.70.16\n",
            "    \u001b[32m# via datasets\u001b[0m\u001b[0m\n",
            "natsort==8.4.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "networkx==3.4.2\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "numba==0.60.0\n",
            "    \u001b[32m# via\n",
            "    #   librosa\n",
            "    #   pynndescent\n",
            "    #   resampy\n",
            "    #   umap-learn\u001b[0m\u001b[0m\n",
            "numpy==1.26.4\n",
            "    \u001b[32m# via\n",
            "    #   ctranslate2\n",
            "    #   datasets\n",
            "    #   einx\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   gradio\n",
            "    #   kaldiio\n",
            "    #   librosa\n",
            "    #   numba\n",
            "    #   onnxruntime\n",
            "    #   pandas\n",
            "    #   pytorch-wpe\n",
            "    #   resampy\n",
            "    #   scikit-learn\n",
            "    #   scipy\n",
            "    #   soxr\n",
            "    #   tensorboard\n",
            "    #   tensorboardx\n",
            "    #   torch-complex\n",
            "    #   torchmetrics\n",
            "    #   transformers\n",
            "    #   umap-learn\u001b[0m\u001b[0m\n",
            "nvidia-cublas-cu12==12.4.5.8\n",
            "    \u001b[32m# via\n",
            "    #   nvidia-cudnn-cu12\n",
            "    #   nvidia-cusolver-cu12\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "nvidia-cuda-cupti-cu12==12.4.127\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cuda-runtime-cu12==12.4.127\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cudnn-cu12==9.1.0.70\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cufft-cu12==11.2.1.3\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-curand-cu12==10.3.5.147\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cusolver-cu12==11.6.1.9\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-cusparse-cu12==12.3.1.170\n",
            "    \u001b[32m# via\n",
            "    #   nvidia-cusolver-cu12\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "nvidia-nvjitlink-cu12==12.4.127\n",
            "    \u001b[32m# via\n",
            "    #   nvidia-cusolver-cu12\n",
            "    #   nvidia-cusparse-cu12\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "omegaconf==2.3.0\n",
            "    \u001b[32m# via hydra-core\u001b[0m\u001b[0m\n",
            "onnxruntime==1.20.1\n",
            "    \u001b[32m# via\n",
            "    #   faster-whisper\n",
            "    #   silero-vad\u001b[0m\u001b[0m\n",
            "opencc-python-reimplemented==0.1.7\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "orjson==3.10.12\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "ormsgpack==1.7.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "oss2==2.19.1\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "packaging==24.2\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   gradio\n",
            "    #   gradio-client\n",
            "    #   huggingface-hub\n",
            "    #   hydra-core\n",
            "    #   lazy-loader\n",
            "    #   lightning\n",
            "    #   lightning-utilities\n",
            "    #   onnxruntime\n",
            "    #   pooch\n",
            "    #   pytorch-lightning\n",
            "    #   tensorboard\n",
            "    #   tensorboardx\n",
            "    #   torch-complex\n",
            "    #   torchmetrics\n",
            "    #   transformers\u001b[0m\u001b[0m\n",
            "pandas==2.2.3\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   gradio\u001b[0m\u001b[0m\n",
            "pillow==11.0.0\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "platformdirs==4.3.6\n",
            "    \u001b[32m# via\n",
            "    #   pooch\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "pooch==1.8.2\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "propcache==0.2.1\n",
            "    \u001b[32m# via\n",
            "    #   aiohttp\n",
            "    #   yarl\u001b[0m\u001b[0m\n",
            "protobuf==5.29.2\n",
            "    \u001b[32m# via\n",
            "    #   onnxruntime\n",
            "    #   tensorboard\n",
            "    #   tensorboardx\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "psutil==6.1.1\n",
            "    \u001b[32m# via wandb\u001b[0m\u001b[0m\n",
            "pyarrow==18.1.0\n",
            "    \u001b[32m# via datasets\u001b[0m\u001b[0m\n",
            "pyarrow-hotfix==0.6\n",
            "    \u001b[32m# via datasets\u001b[0m\u001b[0m\n",
            "pyaudio==0.2.14\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "pycparser==2.22\n",
            "    \u001b[32m# via cffi\u001b[0m\u001b[0m\n",
            "pycryptodome==3.21.0\n",
            "    \u001b[32m# via oss2\u001b[0m\u001b[0m\n",
            "pydantic==2.9.2\n",
            "    \u001b[32m# via\n",
            "    #   fastapi\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   gradio\n",
            "    #   kui\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "pydantic-core==2.23.4\n",
            "    \u001b[32m# via pydantic\u001b[0m\u001b[0m\n",
            "pydub==0.25.1\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   gradio\u001b[0m\u001b[0m\n",
            "pygments==2.18.0\n",
            "    \u001b[32m# via rich\u001b[0m\u001b[0m\n",
            "pynndescent==0.5.13\n",
            "    \u001b[32m# via umap-learn\u001b[0m\u001b[0m\n",
            "pyrootutils==1.0.4\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "python-dateutil==2.9.0.post0\n",
            "    \u001b[32m# via pandas\u001b[0m\u001b[0m\n",
            "python-dotenv==1.0.1\n",
            "    \u001b[32m# via pyrootutils\u001b[0m\u001b[0m\n",
            "python-multipart==0.0.20\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "pytorch-lightning==2.5.0.post0\n",
            "    \u001b[32m# via lightning\u001b[0m\u001b[0m\n",
            "pytorch-wpe==0.0.1\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "pytz==2024.2\n",
            "    \u001b[32m# via pandas\u001b[0m\u001b[0m\n",
            "pyyaml==6.0.2\n",
            "    \u001b[32m# via\n",
            "    #   ctranslate2\n",
            "    #   datasets\n",
            "    #   funasr\n",
            "    #   gradio\n",
            "    #   huggingface-hub\n",
            "    #   lightning\n",
            "    #   omegaconf\n",
            "    #   pytorch-lightning\n",
            "    #   transformers\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "regex==2024.11.6\n",
            "    \u001b[32m# via\n",
            "    #   tiktoken\n",
            "    #   transformers\u001b[0m\u001b[0m\n",
            "requests==2.32.3\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   funasr\n",
            "    #   huggingface-hub\n",
            "    #   modelscope\n",
            "    #   oss2\n",
            "    #   pooch\n",
            "    #   tiktoken\n",
            "    #   transformers\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "resampy==0.4.3\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "rich==13.9.4\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   typer\u001b[0m\u001b[0m\n",
            "ruff==0.8.4\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "safehttpx==0.1.6\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "safetensors==0.4.5\n",
            "    \u001b[32m# via transformers\u001b[0m\u001b[0m\n",
            "scikit-learn==1.6.0\n",
            "    \u001b[32m# via\n",
            "    #   librosa\n",
            "    #   pynndescent\n",
            "    #   umap-learn\u001b[0m\u001b[0m\n",
            "scipy==1.14.1\n",
            "    \u001b[32m# via\n",
            "    #   funasr\n",
            "    #   librosa\n",
            "    #   pynndescent\n",
            "    #   scikit-learn\n",
            "    #   umap-learn\u001b[0m\u001b[0m\n",
            "semantic-version==2.10.0\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "sentencepiece==0.2.0\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "sentry-sdk==2.19.2\n",
            "    \u001b[32m# via wandb\u001b[0m\u001b[0m\n",
            "setproctitle==1.3.4\n",
            "    \u001b[32m# via wandb\u001b[0m\u001b[0m\n",
            "shellingham==1.5.4\n",
            "    \u001b[32m# via typer\u001b[0m\u001b[0m\n",
            "silero-vad==5.1.2\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "six==1.17.0\n",
            "    \u001b[32m# via\n",
            "    #   docker-pycreds\n",
            "    #   oss2\n",
            "    #   python-dateutil\n",
            "    #   tensorboard\u001b[0m\u001b[0m\n",
            "smmap==5.0.1\n",
            "    \u001b[32m# via gitdb\u001b[0m\u001b[0m\n",
            "sniffio==1.3.1\n",
            "    \u001b[32m# via anyio\u001b[0m\u001b[0m\n",
            "soundfile==0.12.1\n",
            "    \u001b[32m# via\n",
            "    #   funasr\n",
            "    #   librosa\u001b[0m\u001b[0m\n",
            "soxr==0.5.0.post1\n",
            "    \u001b[32m# via librosa\u001b[0m\u001b[0m\n",
            "starlette==0.41.3\n",
            "    \u001b[32m# via\n",
            "    #   fastapi\n",
            "    #   gradio\u001b[0m\u001b[0m\n",
            "sympy==1.13.1\n",
            "    \u001b[32m# via\n",
            "    #   einx\n",
            "    #   onnxruntime\n",
            "    #   torch\u001b[0m\u001b[0m\n",
            "tensorboard==2.18.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "tensorboard-data-server==0.7.2\n",
            "    \u001b[32m# via tensorboard\u001b[0m\u001b[0m\n",
            "tensorboardx==2.6.2.2\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "threadpoolctl==3.5.0\n",
            "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
            "tiktoken==0.8.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "tokenizers==0.21.0\n",
            "    \u001b[32m# via\n",
            "    #   faster-whisper\n",
            "    #   transformers\u001b[0m\u001b[0m\n",
            "tomlkit==0.13.2\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "torch==2.5.1\n",
            "    \u001b[32m# via\n",
            "    #   einx\n",
            "    #   lightning\n",
            "    #   pytorch-lightning\n",
            "    #   silero-vad\n",
            "    #   torchaudio\n",
            "    #   torchmetrics\n",
            "    #   vector-quantize-pytorch\u001b[0m\u001b[0m\n",
            "torch-complex==0.4.4\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "torchaudio==2.5.1\n",
            "    \u001b[32m# via silero-vad\u001b[0m\u001b[0m\n",
            "torchmetrics==1.6.0\n",
            "    \u001b[32m# via\n",
            "    #   lightning\n",
            "    #   pytorch-lightning\u001b[0m\u001b[0m\n",
            "tqdm==4.67.1\n",
            "    \u001b[32m# via\n",
            "    #   datasets\n",
            "    #   faster-whisper\n",
            "    #   funasr\n",
            "    #   huggingface-hub\n",
            "    #   lightning\n",
            "    #   modelscope\n",
            "    #   pytorch-lightning\n",
            "    #   transformers\n",
            "    #   umap-learn\u001b[0m\u001b[0m\n",
            "transformers==4.47.1\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "triton==3.1.0\n",
            "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
            "typer==0.15.1\n",
            "    \u001b[32m# via gradio\u001b[0m\u001b[0m\n",
            "typing-extensions==4.12.2\n",
            "    \u001b[32m# via\n",
            "    #   anyio\n",
            "    #   fastapi\n",
            "    #   gradio\n",
            "    #   gradio-client\n",
            "    #   huggingface-hub\n",
            "    #   kui\n",
            "    #   librosa\n",
            "    #   lightning\n",
            "    #   lightning-utilities\n",
            "    #   multidict\n",
            "    #   pydantic\n",
            "    #   pydantic-core\n",
            "    #   pytorch-lightning\n",
            "    #   rich\n",
            "    #   torch\n",
            "    #   typer\n",
            "    #   uvicorn\n",
            "    #   wandb\u001b[0m\u001b[0m\n",
            "tzdata==2024.2\n",
            "    \u001b[32m# via pandas\u001b[0m\u001b[0m\n",
            "umap-learn==0.5.7\n",
            "    \u001b[32m# via funasr\u001b[0m\u001b[0m\n",
            "urllib3==2.3.0\n",
            "    \u001b[32m# via\n",
            "    #   modelscope\n",
            "    #   requests\n",
            "    #   sentry-sdk\u001b[0m\u001b[0m\n",
            "uvicorn==0.34.0\n",
            "    \u001b[32m# via\n",
            "    #   fish-speech (fish-speech/pyproject.toml)\n",
            "    #   gradio\u001b[0m\u001b[0m\n",
            "vector-quantize-pytorch==1.14.24\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "wandb==0.19.1\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "websockets==14.1\n",
            "    \u001b[32m# via gradio-client\u001b[0m\u001b[0m\n",
            "werkzeug==3.1.3\n",
            "    \u001b[32m# via tensorboard\u001b[0m\u001b[0m\n",
            "xxhash==3.5.0\n",
            "    \u001b[32m# via datasets\u001b[0m\u001b[0m\n",
            "yarl==1.18.3\n",
            "    \u001b[32m# via aiohttp\u001b[0m\u001b[0m\n",
            "zstandard==0.23.0\n",
            "    \u001b[32m# via fish-speech (fish-speech/pyproject.toml)\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[32m# The following packages are considered to be unsafe in a requirements file:\u001b[0m\u001b[0m\n",
            "\u001b[32m# setuptools\u001b[0m\u001b[0m\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'fish-speech/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install portaudio\n",
        "!sudo apt-get install gcc\n",
        "!sudo apt-get install portaudio19-dev python3-dev"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9hTamt1QVuHW",
        "outputId": "4aecc84d-8ee6-468a-aaa2-34e8a2b27e31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package portaudio\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "python3-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (167 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EtsdpGjFUlS5",
        "outputId": "f0722036-7208-44fd-c8f3-251d768ffc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==2.1.0 (from -r requirements.txt (line 7))\n",
            "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting aiofiles==23.2.1 (from -r requirements.txt (line 9))\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs==2.4.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.4.4)\n",
            "Collecting aiohttp==3.11.11 (from -r requirements.txt (line 13))\n",
            "  Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: aiosignal==1.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (1.3.2)\n",
            "Collecting aliyun-python-sdk-core==2.16.0 (from -r requirements.txt (line 19))\n",
            "  Using cached aliyun_python_sdk_core-2.16.0-py3-none-any.whl\n",
            "Collecting aliyun-python-sdk-kms==2.16.5 (from -r requirements.txt (line 23))\n",
            "  Using cached aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.7.0)\n",
            "Collecting antlr4-python3-runtime==4.9.3 (from -r requirements.txt (line 27))\n",
            "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
            "Collecting anyio==4.7.0 (from -r requirements.txt (line 31))\n",
            "  Using cached anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting async-timeout==5.0.1 (from -r requirements.txt (line 36))\n",
            "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: attrs==24.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (24.3.0)\n",
            "Requirement already satisfied: audioread==3.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (3.0.1)\n",
            "Collecting av==14.0.1 (from -r requirements.txt (line 42))\n",
            "  Using cached av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting baize==0.22.2 (from -r requirements.txt (line 44))\n",
            "  Using cached baize-0.22.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: cachetools==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (5.5.0)\n",
            "Requirement already satisfied: certifi==2024.12.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (2024.12.14)\n",
            "Requirement already satisfied: cffi==1.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (3.4.0)\n",
            "Collecting click==8.1.8 (from -r requirements.txt (line 60))\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting coloredlogs==15.0.1 (from -r requirements.txt (line 65))\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting crcmod==1.7 (from -r requirements.txt (line 67))\n",
            "  Using cached crcmod-1.7-cp310-cp310-linux_x86_64.whl\n",
            "Collecting cryptography==44.0.0 (from -r requirements.txt (line 69))\n",
            "  Using cached cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting ctranslate2==4.5.0 (from -r requirements.txt (line 71))\n",
            "  Using cached ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting datasets==2.18.0 (from -r requirements.txt (line 73))\n",
            "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting decorator==5.1.1 (from -r requirements.txt (line 75))\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting dill==0.3.8 (from -r requirements.txt (line 77))\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 81)) (0.4.0)\n",
            "Requirement already satisfied: editdistance==0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 83)) (0.8.1)\n",
            "Requirement already satisfied: einops==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 85)) (0.8.0)\n",
            "Collecting einx==0.2.2 (from einx[torch]==0.2.2->-r requirements.txt (line 89))\n",
            "  Using cached einx-0.2.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: exceptiongroup==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 93)) (1.2.2)\n",
            "Collecting fastapi==0.115.6 (from -r requirements.txt (line 95))\n",
            "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting faster-whisper==1.1.0 (from -r requirements.txt (line 97))\n",
            "  Using cached faster_whisper-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ffmpy==0.5.0 (from -r requirements.txt (line 99))\n",
            "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 101)) (3.16.1)\n",
            "Collecting flatbuffers==24.12.23 (from -r requirements.txt (line 108))\n",
            "  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
            "Requirement already satisfied: frozendict==2.4.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 110)) (2.4.6)\n",
            "Requirement already satisfied: frozenlist==1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 112)) (1.5.0)\n",
            "Collecting fsspec==2024.2.0 (from fsspec[http]==2024.2.0->-r requirements.txt (line 116))\n",
            "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting funasr==1.1.5 (from -r requirements.txt (line 124))\n",
            "  Using cached funasr-1.1.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: gitdb==4.0.11 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 126)) (4.0.11)\n",
            "Requirement already satisfied: gitpython==3.1.43 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 128)) (3.1.43)\n",
            "Collecting gradio==5.9.1 (from -r requirements.txt (line 130))\n",
            "  Using cached gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gradio-client==1.5.2 (from -r requirements.txt (line 132))\n",
            "  Using cached gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: grpcio==1.68.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 134)) (1.68.1)\n",
            "Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 138)) (0.14.0)\n",
            "Requirement already satisfied: httpcore==1.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 142)) (1.0.7)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 144)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub==0.27.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 149)) (0.27.0)\n",
            "Collecting humanfriendly==10.0 (from -r requirements.txt (line 157))\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting hydra-core==1.3.2 (from -r requirements.txt (line 159))\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 163)) (3.10)\n",
            "Collecting jaconv==0.4.0 (from -r requirements.txt (line 169))\n",
            "  Using cached jaconv-0.4.0-py3-none-any.whl\n",
            "Collecting jamo==0.4.1 (from -r requirements.txt (line 171))\n",
            "  Using cached jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: jieba==0.42.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 173)) (0.42.1)\n",
            "Collecting jinja2==3.1.5 (from -r requirements.txt (line 175))\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jmespath==0.10.0 (from -r requirements.txt (line 179))\n",
            "  Using cached jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 181)) (1.4.2)\n",
            "Collecting kaldiio==2.18.0 (from -r requirements.txt (line 186))\n",
            "  Using cached kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting kui==1.8.1 (from -r requirements.txt (line 188))\n",
            "  Using cached kui-1.8.1-py3-none-any.whl.metadata (984 bytes)\n",
            "Requirement already satisfied: lazy-loader==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 190)) (0.4)\n",
            "Requirement already satisfied: librosa==0.10.2.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 192)) (0.10.2.post1)\n",
            "Collecting lightning==2.5.0.post0 (from -r requirements.txt (line 196))\n",
            "  Using cached lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "Collecting lightning-utilities==0.11.9 (from -r requirements.txt (line 198))\n",
            "  Using cached lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: llvmlite==0.43.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 203)) (0.43.0)\n",
            "Collecting loguru==0.7.3 (from -r requirements.txt (line 207))\n",
            "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting loralib==0.1.2 (from -r requirements.txt (line 209))\n",
            "  Using cached loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: markdown==3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 211)) (3.7)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 213)) (3.0.0)\n",
            "Collecting markupsafe==2.1.5 (from -r requirements.txt (line 215))\n",
            "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 220)) (0.1.2)\n",
            "Collecting modelscope==1.17.1 (from -r requirements.txt (line 222))\n",
            "  Using cached modelscope-1.17.1-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 224)) (1.3.0)\n",
            "Requirement already satisfied: msgpack==1.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 226)) (1.1.0)\n",
            "Requirement already satisfied: multidict==6.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 228)) (6.1.0)\n",
            "Collecting multiprocess==0.70.16 (from -r requirements.txt (line 232))\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 234)) (8.4.0)\n",
            "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 236)) (3.4.2)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 238)) (0.60.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 244)) (1.26.4)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from -r requirements.txt (line 267))\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from -r requirements.txt (line 272))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from -r requirements.txt (line 274))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from -r requirements.txt (line 276))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from -r requirements.txt (line 278))\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from -r requirements.txt (line 280))\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from -r requirements.txt (line 282))\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from -r requirements.txt (line 284))\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from -r requirements.txt (line 286))\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from -r requirements.txt (line 290))\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from -r requirements.txt (line 292))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from -r requirements.txt (line 297))\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting omegaconf==2.3.0 (from -r requirements.txt (line 299))\n",
            "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting onnxruntime==1.20.1 (from -r requirements.txt (line 301))\n",
            "  Using cached onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opencc-python-reimplemented==0.1.7 (from -r requirements.txt (line 305))\n",
            "  Using cached opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: orjson==3.10.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 307)) (3.10.12)\n",
            "Collecting ormsgpack==1.7.0 (from -r requirements.txt (line 309))\n",
            "  Using cached ormsgpack-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "Collecting oss2==2.19.1 (from -r requirements.txt (line 311))\n",
            "  Using cached oss2-2.19.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 313)) (24.2)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 331))\n",
            "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 335)) (11.0.0)\n",
            "Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 337)) (4.3.6)\n",
            "Requirement already satisfied: pooch==1.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 341)) (1.8.2)\n",
            "Requirement already satisfied: propcache==0.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 343)) (0.2.1)\n",
            "Collecting protobuf==5.29.2 (from -r requirements.txt (line 347))\n",
            "  Using cached protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting psutil==6.1.1 (from -r requirements.txt (line 353))\n",
            "  Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting pyarrow==18.1.0 (from -r requirements.txt (line 355))\n",
            "  Using cached pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pyarrow-hotfix==0.6 (from -r requirements.txt (line 357))\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pyaudio==0.2.14 (from -r requirements.txt (line 359))\n",
            "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycparser==2.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 361)) (2.22)\n",
            "Collecting pycryptodome==3.21.0 (from -r requirements.txt (line 363))\n",
            "  Using cached pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pydantic==2.9.2 (from -r requirements.txt (line 365))\n",
            "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "Collecting pydantic-core==2.23.4 (from -r requirements.txt (line 372))\n",
            "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pydub==0.25.1 (from -r requirements.txt (line 374))\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 378)) (2.18.0)\n",
            "Collecting pynndescent==0.5.13 (from -r requirements.txt (line 380))\n",
            "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting pyrootutils==1.0.4 (from -r requirements.txt (line 382))\n",
            "  Using cached pyrootutils-1.0.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 384))\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 386))\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting python-multipart==0.0.20 (from -r requirements.txt (line 388))\n",
            "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pytorch-lightning==2.5.0.post0 (from -r requirements.txt (line 390))\n",
            "  Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch-wpe==0.0.1 (from -r requirements.txt (line 392))\n",
            "  Using cached pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\n",
            "Requirement already satisfied: pytz==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 394)) (2024.2)\n",
            "Requirement already satisfied: pyyaml==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 396)) (6.0.2)\n",
            "Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 408)) (2024.11.6)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 412)) (2.32.3)\n",
            "Collecting resampy==0.4.3 (from -r requirements.txt (line 423))\n",
            "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: rich==13.9.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 425)) (13.9.4)\n",
            "Collecting ruff==0.8.4 (from -r requirements.txt (line 429))\n",
            "  Using cached ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx==0.1.6 (from -r requirements.txt (line 431))\n",
            "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: safetensors==0.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 433)) (0.4.5)\n",
            "Requirement already satisfied: scikit-learn==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 435)) (1.6.0)\n",
            "Collecting scipy==1.14.1 (from -r requirements.txt (line 440))\n",
            "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting semantic-version==2.10.0 (from -r requirements.txt (line 447))\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 449)) (0.2.0)\n",
            "Requirement already satisfied: sentry-sdk==2.19.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 451)) (2.19.2)\n",
            "Requirement already satisfied: setproctitle==1.3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 453)) (1.3.4)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 455)) (1.5.4)\n",
            "Collecting silero-vad==5.1.2 (from -r requirements.txt (line 457))\n",
            "  Using cached silero_vad-5.1.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 459)) (1.17.0)\n",
            "Requirement already satisfied: smmap==5.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 465)) (5.0.1)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 467)) (1.3.1)\n",
            "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 469)) (0.12.1)\n",
            "Requirement already satisfied: soxr==0.5.0.post1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 473)) (0.5.0.post1)\n",
            "Collecting starlette==0.41.3 (from -r requirements.txt (line 475))\n",
            "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 479)) (1.13.1)\n",
            "Collecting tensorboard==2.18.0 (from -r requirements.txt (line 484))\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorboard-data-server==0.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 486)) (0.7.2)\n",
            "Collecting tensorboardx==2.6.2.2 (from -r requirements.txt (line 488))\n",
            "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 490)) (3.5.0)\n",
            "Collecting tiktoken==0.8.0 (from -r requirements.txt (line 492))\n",
            "  Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 494)) (0.21.0)\n",
            "Collecting tomlkit==0.13.2 (from -r requirements.txt (line 498))\n",
            "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 500)) (2.5.1+cu121)\n",
            "Collecting torch-complex==0.4.4 (from -r requirements.txt (line 509))\n",
            "  Using cached torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 511)) (2.5.1+cu121)\n",
            "Collecting torchmetrics==1.6.0 (from -r requirements.txt (line 513))\n",
            "  Using cached torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 517)) (4.67.1)\n",
            "Requirement already satisfied: transformers==4.47.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 528)) (4.47.1)\n",
            "Collecting triton==3.1.0 (from -r requirements.txt (line 530))\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: typer==0.15.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 532)) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 534)) (4.12.2)\n",
            "Requirement already satisfied: tzdata==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 554)) (2024.2)\n",
            "Collecting umap-learn==0.5.7 (from -r requirements.txt (line 556))\n",
            "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting urllib3==2.3.0 (from -r requirements.txt (line 558))\n",
            "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting uvicorn==0.34.0 (from -r requirements.txt (line 563))\n",
            "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting vector-quantize-pytorch==1.14.24 (from -r requirements.txt (line 567))\n",
            "  Using cached vector_quantize_pytorch-1.14.24-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: wandb==0.19.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 569)) (0.19.1)\n",
            "Requirement already satisfied: websockets==14.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 571)) (14.1)\n",
            "Requirement already satisfied: werkzeug==3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 573)) (3.1.3)\n",
            "Collecting xxhash==3.5.0 (from -r requirements.txt (line 575))\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: yarl==1.18.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 577)) (1.18.3)\n",
            "Collecting zstandard==0.23.0 (from -r requirements.txt (line 579))\n",
            "  Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2==4.5.0->-r requirements.txt (line 71)) (75.1.0)\n",
            "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Using cached aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "Using cached anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Using cached av-14.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "Using cached baize-0.22.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (743 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "Using cached ctranslate2-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached einx-0.2.2-py3-none-any.whl (101 kB)\n",
            "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "Using cached faster_whisper-1.1.0-py3-none-any.whl (1.1 MB)\n",
            "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Using cached flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
            "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "Using cached funasr-1.1.5-py3-none-any.whl (649 kB)\n",
            "Using cached gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "Using cached gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Using cached jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Using cached kui-1.8.1-py3-none-any.whl (65 kB)\n",
            "Using cached lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "Using cached lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Using cached loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached modelscope-1.17.1-py3-none-any.whl (5.7 MB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Using cached onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "Using cached opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "Using cached ormsgpack-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Using cached psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "Using cached pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "Using cached pyrootutils-1.0.4-py3-none-any.whl (5.8 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Using cached pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "Using cached pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "Using cached ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Using cached silero_vad-5.1.2-py3-none-any.whl (5.0 MB)\n",
            "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Using cached torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\n",
            "Using cached torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Using cached vector_quantize_pytorch-1.14.24-py3-none-any.whl (36 kB)\n",
            "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Using cached zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=PyAudio-0.2.14-cp310-cp310-linux_x86_64.whl size=63864 sha256=0ac217f029bab4f3c57451d2cd9564c094c6dd853e86a4f4be534de3bbea0167\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/21/f4/0b51d41ba79e51b16295cbb096ec49f334792814d545b508c5\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pydub, pyaudio, opencc-python-reimplemented, jamo, jaconv, flatbuffers, crcmod, antlr4-python3-runtime, zstandard, xxhash, urllib3, triton, torch-complex, tomlkit, semantic-version, scipy, ruff, pytorch-wpe, python-multipart, python-dotenv, python-dateutil, pydantic-core, pycryptodome, pyarrow-hotfix, pyarrow, psutil, protobuf, ormsgpack, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, loralib, loguru, lightning-utilities, kaldiio, jmespath, humanfriendly, fsspec, ffmpy, dill, decorator, ctranslate2, click, baize, av, async-timeout, anyio, aiofiles, absl-py, uvicorn, tensorboardx, starlette, resampy, pyrootutils, pydantic, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, hydra-core, einx, cryptography, coloredlogs, tiktoken, tensorboard, safehttpx, pynndescent, onnxruntime, nvidia-cusolver-cu12, modelscope, kui, fastapi, aliyun-python-sdk-core, aiohttp, vector-quantize-pytorch, umap-learn, torchmetrics, gradio-client, aliyun-python-sdk-kms, silero-vad, pytorch-lightning, oss2, gradio, faster-whisper, datasets, lightning, funasr\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.1\n",
            "    Uninstalling pydantic_core-2.27.1:\n",
            "      Successfully uninstalled pydantic_core-2.27.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.3\n",
            "    Uninstalling async-timeout-4.0.3:\n",
            "      Successfully uninstalled async-timeout-4.0.3\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.3\n",
            "    Uninstalling pydantic-2.10.3:\n",
            "      Successfully uninstalled pydantic-2.10.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.10\n",
            "    Uninstalling aiohttp-3.11.10:\n",
            "      Successfully uninstalled aiohttp-3.11.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.1.0 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.2.0 which is incompatible.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\n",
            "langchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.0 which is incompatible.\n",
            "pylibcudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.1.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.1.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.1.0 aiofiles-23.2.1 aiohttp-3.11.11 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 antlr4-python3-runtime-4.9.3 anyio-4.7.0 async-timeout-5.0.1 av-14.0.1 baize-0.22.2 click-8.1.8 coloredlogs-15.0.1 crcmod-1.7 cryptography-44.0.0 ctranslate2-4.5.0 datasets-2.18.0 decorator-5.1.1 dill-0.3.8 einx-0.2.2 fastapi-0.115.6 faster-whisper-1.1.0 ffmpy-0.5.0 flatbuffers-24.12.23 fsspec-2024.2.0 funasr-1.1.5 gradio-5.9.1 gradio-client-1.5.2 humanfriendly-10.0 hydra-core-1.3.2 jaconv-0.4.0 jamo-0.4.1 jinja2-3.1.5 jmespath-0.10.0 kaldiio-2.18.0 kui-1.8.1 lightning-2.5.0.post0 lightning-utilities-0.11.9 loguru-0.7.3 loralib-0.1.2 markupsafe-2.1.5 modelscope-1.17.1 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 onnxruntime-1.20.1 opencc-python-reimplemented-0.1.7 ormsgpack-1.7.0 oss2-2.19.1 pandas-2.2.3 protobuf-5.29.2 psutil-6.1.1 pyarrow-18.1.0 pyarrow-hotfix-0.6 pyaudio-0.2.14 pycryptodome-3.21.0 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 pynndescent-0.5.13 pyrootutils-1.0.4 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 python-multipart-0.0.20 pytorch-lightning-2.5.0.post0 pytorch-wpe-0.0.1 resampy-0.4.3 ruff-0.8.4 safehttpx-0.1.6 scipy-1.14.1 semantic-version-2.10.0 silero-vad-5.1.2 starlette-0.41.3 tensorboard-2.18.0 tensorboardx-2.6.2.2 tiktoken-0.8.0 tomlkit-0.13.2 torch-complex-0.4.4 torchmetrics-1.6.0 triton-3.1.0 umap-learn-0.5.7 urllib3-2.3.0 uvicorn-0.34.0 vector-quantize-pytorch-1.14.24 xxhash-3.5.0 zstandard-0.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "decorator",
                  "psutil",
                  "pyarrow",
                  "pydevd_plugins"
                ]
              },
              "id": "946a36957b194783a8cfbb47a6c3e581"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Ym-GRTmAXEDf",
        "outputId": "b2608a9a-67ec-47de-fd82-50ac27a65deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWiu3zLPMXZ"
      },
      "source": [
        "### For Windows User / win用户"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "id": "JUan7RDmPMXZ"
      },
      "outputs": [],
      "source": [
        "!chcp 65001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76qmhNSPMXa"
      },
      "source": [
        "### For Linux User / Linux 用户"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IqrVmVhaPMXa",
        "outputId": "b33f4172-8e3a-4691-85f5-87de8bde574c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaCru_zIPMXa"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5P7b3sPvPMXa",
        "outputId": "f5a8ef44-01ea-4d97-b538-796322416262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 7 files:   0% 0/7 [00:00<?, ?it/s]Downloading '.gitattributes' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/.gitattributes.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'config.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/config.json.3f8edf91f7a0b152e5f8c30fd412c5d7e22020b5.incomplete'\n",
            "Downloading 'model.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/model.pth.918dc960372cc1b77bbafb14c48ef7a1634ecf75d4eb85b78607223b780d6001.incomplete'\n",
            "Downloading 'firefly-gan-vq-fsq-8x1024-21hz-generator.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/firefly-gan-vq-fsq-8x1024-21hz-generator.pth.01b81dbf753224a156c3fe139b88bf0b9a0f54b11bee864f95e66511c3ccd754.incomplete'\n",
            "Downloading 'tokenizer.tiktoken' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/tokenizer.tiktoken.21dcfcb37df8da533b2d4fe0b867472f04cda62e.incomplete'\n",
            "Downloading 'special_tokens.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/special_tokens.json.db54e3cccbbaa1106ba8d56e810dffd42e325ab0.incomplete'\n",
            "Downloading 'README.md' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/README.md.a43b65346beef6fa83135bd1dff857a00b6a9c13.incomplete'\n",
            "\n",
            "\r.gitattributes:   0% 0.00/1.52k [00:00<?, ?B/s]\u001b[A\r.gitattributes: 100% 1.52k/1.52k [00:00<00:00, 13.7MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/.gitattributes\n",
            "\rFetching 7 files:  14% 1/7 [00:00<00:01,  5.04it/s]\n",
            "\rconfig.json:   0% 0.00/697 [00:00<?, ?B/s]\u001b[A\rconfig.json: 100% 697/697 [00:00<00:00, 7.25MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/config.json\n",
            "\n",
            "\rtokenizer.tiktoken:   0% 0.00/1.70M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\rspecial_tokens.json:   0% 0.00/31.0k [00:00<?, ?B/s]\u001b[A\u001b[A\rspecial_tokens.json: 100% 31.0k/31.0k [00:00<00:00, 5.85MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/special_tokens.json\n",
            "\n",
            "\n",
            "README.md: 100% 1.68k/1.68k [00:00<00:00, 4.56MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/README.md\n",
            "\n",
            "\n",
            "model.pth:   0% 0.00/1.28G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   0% 0.00/189M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "tokenizer.tiktoken: 100% 1.70M/1.70M [00:00<00:00, 8.66MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/tokenizer.tiktoken\n",
            "\n",
            "\n",
            "model.pth:   1% 10.5M/1.28G [00:00<00:30, 40.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   6% 10.5M/189M [00:00<00:04, 39.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   2% 21.0M/1.28G [00:00<00:30, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  11% 21.0M/189M [00:00<00:04, 40.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   2% 31.5M/1.28G [00:00<00:29, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  17% 31.5M/189M [00:00<00:03, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   3% 41.9M/1.28G [00:00<00:28, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  22% 41.9M/189M [00:01<00:03, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   4% 52.4M/1.28G [00:01<00:28, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  28% 52.4M/189M [00:01<00:03, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   5% 62.9M/1.28G [00:01<00:28, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  33% 62.9M/189M [00:01<00:02, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   6% 73.4M/1.28G [00:01<00:27, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  39% 73.4M/189M [00:01<00:02, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   7% 83.9M/1.28G [00:01<00:27, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  44% 83.9M/189M [00:01<00:02, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   7% 94.4M/1.28G [00:02<00:27, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  50% 94.4M/189M [00:02<00:02, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   8% 105M/1.28G [00:02<00:27, 42.8MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  56% 105M/189M [00:02<00:01, 42.5MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:   9% 115M/1.28G [00:02<00:27, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  61% 115M/189M [00:02<00:01, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  10% 126M/1.28G [00:02<00:27, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  67% 126M/189M [00:03<00:03, 20.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  11% 136M/1.28G [00:05<01:51, 10.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  72% 136M/189M [00:05<00:04, 11.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  83% 157M/189M [00:06<00:01, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  12% 157M/1.28G [00:06<01:06, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  89% 168M/189M [00:06<00:00, 21.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  13% 168M/1.28G [00:06<00:55, 19.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  95% 178M/189M [00:06<00:00, 25.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  14% 178M/1.28G [00:06<00:47, 23.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth: 100% 189M/189M [00:06<00:00, 27.9MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\n",
            "Fetching 7 files:  57% 4/7 [00:07<00:05,  1.88s/it]\n",
            "\n",
            "model.pth:  15% 189M/1.28G [00:06<00:41, 26.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  16% 199M/1.28G [00:07<00:36, 29.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  16% 210M/1.28G [00:07<00:32, 32.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  17% 220M/1.28G [00:07<00:30, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  18% 231M/1.28G [00:07<00:28, 37.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  19% 241M/1.28G [00:08<00:26, 38.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  20% 252M/1.28G [00:08<00:25, 39.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  21% 262M/1.28G [00:08<00:24, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  21% 273M/1.28G [00:08<00:24, 41.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  22% 283M/1.28G [00:09<00:23, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  23% 294M/1.28G [00:09<00:23, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  24% 304M/1.28G [00:09<00:23, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  25% 315M/1.28G [00:09<00:22, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  25% 325M/1.28G [00:10<00:25, 36.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  26% 336M/1.28G [00:10<00:21, 44.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  27% 346M/1.28G [00:10<00:21, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  28% 357M/1.28G [00:10<00:21, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  29% 367M/1.28G [00:10<00:20, 43.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  30% 377M/1.28G [00:11<00:20, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  30% 388M/1.28G [00:11<00:20, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  31% 398M/1.28G [00:11<00:20, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  32% 409M/1.28G [00:11<00:20, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  33% 419M/1.28G [00:12<00:19, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  34% 430M/1.28G [00:12<00:19, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  35% 440M/1.28G [00:12<00:19, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  35% 451M/1.28G [00:12<00:16, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  37% 472M/1.28G [00:13<00:11, 67.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  38% 482M/1.28G [00:13<00:13, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  39% 493M/1.28G [00:13<00:14, 53.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  39% 503M/1.28G [00:13<00:15, 50.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  40% 514M/1.28G [00:14<00:15, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  41% 524M/1.28G [00:14<00:16, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  42% 535M/1.28G [00:14<00:16, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  43% 545M/1.28G [00:14<00:16, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  44% 556M/1.28G [00:14<00:16, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  44% 566M/1.28G [00:15<00:16, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  45% 577M/1.28G [00:15<00:16, 43.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  46% 587M/1.28G [00:15<00:16, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  47% 598M/1.28G [00:15<00:15, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  48% 608M/1.28G [00:16<00:15, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  48% 619M/1.28G [00:16<00:15, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  49% 629M/1.28G [00:16<00:15, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  50% 640M/1.28G [00:16<00:14, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  51% 650M/1.28G [00:17<00:14, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  52% 661M/1.28G [00:17<00:14, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  53% 671M/1.28G [00:17<00:14, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  53% 682M/1.28G [00:17<00:13, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  54% 692M/1.28G [00:18<00:13, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  55% 703M/1.28G [00:18<00:13, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  56% 713M/1.28G [00:18<00:13, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  57% 724M/1.28G [00:18<00:12, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  58% 734M/1.28G [00:19<00:12, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  58% 744M/1.28G [00:19<00:12, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  59% 755M/1.28G [00:19<00:12, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  60% 765M/1.28G [00:19<00:11, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  61% 776M/1.28G [00:20<00:11, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  62% 786M/1.28G [00:20<00:11, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  62% 797M/1.28G [00:20<00:11, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  63% 807M/1.28G [00:20<00:10, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  64% 818M/1.28G [00:21<00:10, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  65% 828M/1.28G [00:21<00:10, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  66% 839M/1.28G [00:21<00:10, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  67% 849M/1.28G [00:21<00:10, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  67% 860M/1.28G [00:22<00:09, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  68% 870M/1.28G [00:22<00:09, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  69% 881M/1.28G [00:22<00:09, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  70% 891M/1.28G [00:22<00:09, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  71% 902M/1.28G [00:23<00:09, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  71% 912M/1.28G [00:23<00:08, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  72% 923M/1.28G [00:23<00:08, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  73% 933M/1.28G [00:23<00:08, 42.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  74% 944M/1.28G [00:24<00:07, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  75% 954M/1.28G [00:24<00:07, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  76% 965M/1.28G [00:24<00:07, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  76% 975M/1.28G [00:24<00:07, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  77% 986M/1.28G [00:25<00:06, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  78% 996M/1.28G [00:25<00:06, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  79% 1.01G/1.28G [00:25<00:06, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  80% 1.02G/1.28G [00:25<00:06, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  81% 1.03G/1.28G [00:26<00:05, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  81% 1.04G/1.28G [00:26<00:05, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  82% 1.05G/1.28G [00:26<00:05, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  83% 1.06G/1.28G [00:26<00:05, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  84% 1.07G/1.28G [00:27<00:04, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  85% 1.08G/1.28G [00:27<00:04, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  85% 1.09G/1.28G [00:27<00:04, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  86% 1.10G/1.28G [00:27<00:04, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  87% 1.11G/1.28G [00:28<00:03, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  88% 1.12G/1.28G [00:28<00:03, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  89% 1.13G/1.28G [00:28<00:03, 42.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  90% 1.14G/1.28G [00:28<00:03, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  90% 1.15G/1.28G [00:29<00:02, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  91% 1.16G/1.28G [00:29<00:02, 42.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  92% 1.17G/1.28G [00:29<00:02, 42.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  93% 1.18G/1.28G [00:29<00:02, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  94% 1.20G/1.28G [00:30<00:01, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  95% 1.21G/1.28G [00:30<00:01, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  95% 1.22G/1.28G [00:30<00:01, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  96% 1.23G/1.28G [00:30<00:01, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  97% 1.24G/1.28G [00:30<00:00, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  98% 1.25G/1.28G [00:31<00:00, 42.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  99% 1.26G/1.28G [00:31<00:00, 42.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth:  99% 1.27G/1.28G [00:31<00:00, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.pth: 100% 1.28G/1.28G [00:31<00:00, 40.0MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/model.pth\n",
            "Fetching 7 files: 100% 7/7 [00:32<00:00,  4.60s/it]\n",
            "/content/checkpoints/fish-speech-1.5\n"
          ]
        }
      ],
      "source": [
        "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
        "# !set HF_ENDPOINT=https://hf-mirror.com\n",
        "# !export HF_ENDPOINT=https://hf-mirror.com\n",
        "!cd fish-speech/\n",
        "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66R07XJPMXa"
      },
      "source": [
        "## WebUI Inference\n",
        "\n",
        "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jefYpUxEPMXa",
        "outputId": "51e89f2a-93f0-4b69-9204-b4ba121ed7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "\u001b[32m2024-12-24 15:52:18.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Llama model...\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:30.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m682\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:30.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m688\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:30.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading VQ-GAN model...\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.10/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.10/dist-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.10/dist-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "\u001b[32m2024-12-24 15:52:32.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:32.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDecoder model loaded, warming up...\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:32.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:32.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
            "  0% 0/1023 [00:00<?, ?it/s]/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  2% 18/1023 [00:02<01:57,  8.58it/s]\n",
            "\u001b[32m2024-12-24 15:52:36.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 20 tokens in 3.80 seconds, 5.26 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:36.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 3.36 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:36.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 1.84 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:36.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 19])\u001b[0m\n",
            "\u001b[32m2024-12-24 15:52:36.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mWarming up done, launching the web UI...\u001b[0m\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://5362071a8101aedcdc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "\u001b[32m2024-12-24 15:54:01.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 77.05 seconds\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/vector_quantize_pytorch/residual_fsq.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled = False):\n",
            "\u001b[32m2024-12-24 15:54:02.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 1659])\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:02.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Meet Anubhav Kumar, the semiconductor wizard dazzling Qualcomm as a Senior Engineer.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:02.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: With a Master's in Microelectronics and VLSI from IIT Roorkee, he's an expert in ASIC design, low power implementation, for sub-10nm SoCs.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:02.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Anubhav's resume reads like a tech thriller, from taming timing ECOs to working on EVMs at BEL.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:02.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: When not in semiconductor design, he leads high-altitude treks and connects students to dream jobs as a former placement coordinator. Ready to interview this multi-talented dynamo?\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:02.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 1/4 of sample 1/1\u001b[0m\n",
            "  0% 0/6474 [00:00<?, ?it/s]/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  3% 179/6474 [00:16<09:31, 11.01it/s]\n",
            "\u001b[32m2024-12-24 15:54:20.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 181 tokens in 18.02 seconds, 10.04 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:20.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.41 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:20.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 4.90 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:20.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 2/4 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:20.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 180])\u001b[0m\n",
            "  5% 312/6240 [00:28<09:10, 10.76it/s]\n",
            "\u001b[32m2024-12-24 15:54:51.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 314 tokens in 31.29 seconds, 10.04 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:51.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.40 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:51.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 7.97 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:51.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 3/4 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:54:51.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 313])\u001b[0m\n",
            "  3% 204/5888 [00:18<08:37, 10.99it/s]\n",
            "\u001b[32m2024-12-24 15:55:13.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 206 tokens in 21.42 seconds, 9.62 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:13.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.14 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:13.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 11.60 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:13.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 4/4 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:13.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 205])\u001b[0m\n",
            "  6% 310/5630 [00:28<08:12, 10.80it/s]\n",
            "\u001b[32m2024-12-24 15:55:44.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 312 tokens in 31.70 seconds, 9.84 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:44.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.28 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:44.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:55:44.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 311])\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py:738: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "\u001b[32m2024-12-24 15:58:17.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.reference_loader\u001b[0m:\u001b[36mload_by_hash\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mUse same references\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Meet Anubhav Kumar, the semiconductor wizard and timing closure maestro!\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Currently dazzling Qualcomm as a Senior Engineer, Anubhav has a knack for making complex VLSI designs look like a walk in the park.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: With a Master's in Microelectronics and VLSI from IIT Roorkee, he's not just book-smart but also a hands-on expert in ASIC design, low power implementation, and sub-10nm SoCs.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Anubhav's resume reads like a tech thriller: from taming timing ECOs to wrestling with IR drops and ESD signoffs, he's done it all.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: He's the guy who ensures your mobile and IoT devices don't just work—they perform like rockstars.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: And let's not forget his stint at Bharat Electronics, where he worked on the logic circuit boards for Electronic Voting Machines. Democracy owes him a high-five!\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: When he's not buried in semiconductor design, Anubhav leads high-altitude treks, scaling peaks that would make most of us break a sweat just thinking about.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: He's also a former placement coordinator, connecting the dots between students and their dream jobs.\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: In short, Anubhav is the tech-savvy, mountain-climbing, problem-solving engineer you didn't know you needed. Ready to dive into the interview with this multi-talented dynamo?\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mEncoded text: Buckle up, it's going to be an enlightening ride!\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:17.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 1/10 of sample 1/1\u001b[0m\n",
            "  3% 170/6479 [00:16<10:07, 10.39it/s]\n",
            "\u001b[32m2024-12-24 15:58:35.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 172 tokens in 18.19 seconds, 9.46 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:35.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.03 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:35.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:35.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 2/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:58:35.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 171])\u001b[0m\n",
            "  5% 311/6262 [00:28<09:13, 10.75it/s]\n",
            "\u001b[32m2024-12-24 15:59:06.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 313 tokens in 31.21 seconds, 10.03 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:06.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.40 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:06.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:06.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 3/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:06.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 312])\u001b[0m\n",
            "  6% 377/5886 [00:35<08:38, 10.63it/s]\n",
            "\u001b[32m2024-12-24 15:59:44.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 379 tokens in 38.39 seconds, 9.87 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:44.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.30 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:44.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:44.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 4/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 15:59:44.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 378])\u001b[0m\n",
            "  5% 285/5460 [00:26<08:04, 10.69it/s]\n",
            "\u001b[32m2024-12-24 16:00:15.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 287 tokens in 30.26 seconds, 9.49 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:15.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.05 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:15.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:15.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 5/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:15.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 286])\u001b[0m\n",
            "  3% 173/5496 [00:16<08:13, 10.79it/s]\n",
            "\u001b[32m2024-12-24 16:00:34.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 175 tokens in 19.40 seconds, 9.02 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:34.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 5.75 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:34.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:34.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 6/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:00:34.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 174])\u001b[0m\n",
            "  5% 263/5270 [00:24<07:44, 10.77it/s]\n",
            "\u001b[32m2024-12-24 16:01:02.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 265 tokens in 27.71 seconds, 9.56 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:02.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.10 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:02.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:02.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 7/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:02.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 264])\u001b[0m\n",
            "  5% 255/5397 [00:23<07:56, 10.79it/s]\n",
            "\u001b[32m2024-12-24 16:01:29.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 257 tokens in 27.03 seconds, 9.51 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:29.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.07 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:29.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:29.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 8/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:29.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 256])\u001b[0m\n",
            "  3% 153/5446 [00:14<08:22, 10.53it/s]\n",
            "\u001b[32m2024-12-24 16:01:47.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 155 tokens in 17.88 seconds, 8.67 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:47.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 5.53 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:47.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:47.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 9/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:01:47.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 154])\u001b[0m\n",
            "  5% 280/5233 [00:26<07:45, 10.65it/s]\n",
            "\u001b[32m2024-12-24 16:02:17.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 282 tokens in 29.77 seconds, 9.47 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:17.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 6.04 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:17.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:17.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m807\u001b[0m - \u001b[1mGenerating sentence 10/10 of sample 1/1\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:17.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 281])\u001b[0m\n",
            "  1% 77/5138 [00:07<08:14, 10.22it/s]\n",
            "\u001b[32m2024-12-24 16:02:28.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m861\u001b[0m - \u001b[1mGenerated 79 tokens in 11.23 seconds, 7.03 tokens/sec\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:28.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m864\u001b[0m - \u001b[1mBandwidth achieved: 4.49 GB/s\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:28.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.llama.generate\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m869\u001b[0m - \u001b[1mGPU Memory used: 15.63 GB\u001b[0m\n",
            "\u001b[32m2024-12-24 16:02:28.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 78])\u001b[0m\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5362071a8101aedcdc.gradio.live\n"
          ]
        }
      ],
      "source": [
        "!python fish-speech/tools/run_webui.py \\\n",
        "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
        "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
        "    # --compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVvgn33PMXa"
      },
      "source": [
        "## Break-down CLI Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBaN2YQGPMXa"
      },
      "source": [
        "### 1. Encode reference audio: / 从语音生成 prompt:\n",
        "\n",
        "You should get a `fake.npy` file.\n",
        "\n",
        "你应该能得到一个 `fake.npy` 文件."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "UjRhior6PMXb"
      },
      "outputs": [],
      "source": [
        "## Enter the path to the audio file here\n",
        "src_audio = r\"D:\\PythonProject\\vo_hutao_draw_appear.wav\"\n",
        "\n",
        "!python tools/vqgan/inference.py \\\n",
        "    -i {src_audio} \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "audio = Audio(filename=\"fake.wav\")\n",
        "display(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEUw_2GPMXb"
      },
      "source": [
        "### 2. Generate semantic tokens from text: / 从文本生成语义 token:\n",
        "\n",
        "> This command will create a codes_N file in the working directory, where N is an integer starting from 0.\n",
        "\n",
        "> You may want to use `--compile` to fuse CUDA kernels for faster inference (~30 tokens/second -> ~300 tokens/second).\n",
        "\n",
        "> 该命令会在工作目录下创建 codes_N 文件, 其中 N 是从 0 开始的整数.\n",
        "\n",
        "> 您可以使用 `--compile` 来融合 cuda 内核以实现更快的推理 (~30 tokens/秒 -> ~300 tokens/秒)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "C4cE_tAwPMXb"
      },
      "outputs": [],
      "source": [
        "!python tools/llama/generate.py \\\n",
        "    --text \"hello world\" \\\n",
        "    --prompt-text \"The text corresponding to reference audio\" \\\n",
        "    --prompt-tokens \"fake.npy\" \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4\" \\\n",
        "    --num-samples 2\n",
        "    # --compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZczelpiiPMXb"
      },
      "source": [
        "### 3. Generate speech from semantic tokens: / 从语义 token 生成人声:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "-jgvg7KsPMXb"
      },
      "outputs": [],
      "source": [
        "!python tools/vqgan/inference.py \\\n",
        "    -i \"codes_0.npy\" \\\n",
        "    --checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\"\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "audio = Audio(filename=\"fake.wav\")\n",
        "display(audio)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}